{
 "cells": [
  {
   "source": [
    "# Welcome to Trio de Informática's Second IART Project about Topic Modelling using NLP\n",
    "\n",
    "Throughout this notebook one can observe the step by step data analysis of the training datasets as well as different approaches to this challenge. Several algorithms will also be implemented such as Naive Bayes, decision trees, SVM, etc. Be aware that different data processing techniques can match different algorithms, so in order to test all the combinations several cells will be provided to present all the results, as well as a comparison of accuracy of each implemented technique. \n",
    "\n",
    "The following notebook will record the group's approach to the proposed project, starting with the data analysis, followed by the data preprocessing. Afterwards, various algorithms will be implemented and a result comparison between different approaches will be performed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Statistical analysis\n",
    "\n",
    "As one can observe when executing the code below, the provided dataset is composed of a table with several columns referring to different informations. Every entry in this dataset is identified by an ID, generated incrementally. Each entry contains a Title and an Abstract, reffering to a report/article to be classified as belonging to some topic. Each entry has an additional 6 columns related to topic classification. Each of these columns reffers to a topic: Computer Science, Physics, Mathematics, Statistics, Quantitative Biology and Quantitative Finance, and will contain a 1 if the article belongs to that said topic and a 0 otherwise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Importing the dataset\n",
    "train = pd.read_csv('archive/train.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "source": [
    "## Size of the training dataset\n",
    "\n",
    "It is important to know the magnitude of the dataset used to train the predictive model. Therefore, the following block of code will allow us to check the dimention of the used dataset. The bigger the dataset, the more data is taken into account when training the model, thus allowing it to acquire more information."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train: Rows & Columns\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1f87aafc4575>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train: Rows & Columns\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Train: Rows & Columns\")\n",
    "print(train.shape)"
   ]
  },
  {
   "source": [
    "## Training Dataset Info\n",
    "\n",
    "By showing the info of the training dataset, one can see that there are no null values, meaning that there is no missing data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "source": [
    "## Dataset description\n",
    "\n",
    "Additionally, as shown below, the training dataset seems very clean because:\n",
    "\n",
    "- Number of rows for all the data columns are same\n",
    "- The max of all these 0-1 inputs is always 1\n",
    "- The min and max value of the column ID match the expected values and so do the 25%, 50% and 75%, thus showing that the ID is incremented correctly\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-39c93376966e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "source": [
    "## Topic Distribution\n",
    "\n",
    "Each entry in the training dataset belongs to a topic. In order to evaluate the topics with the most and least entries, below is presented a numerical analysis where we can find the topic distribution of the training dataset. The model will classify more easily papers related to topics with the most entries in the training dataset, therefore the predictive model will be better trained to identify the more recurring topics of the training dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of articles that belong the the topic:\\n')\n",
    "print('- Computer Science: ',train['Computer Science'].sum())\n",
    "print('- Physics: ',train['Physics'].sum())\n",
    "print('- Mathematics: ',train['Mathematics'].sum())\n",
    "print('- Statistics: ',train['Statistics'].sum())\n",
    "print('- Quantitative Biology: ',train['Quantitative Biology'].sum())\n",
    "print('- Quantitative Finance: ',train['Quantitative Finance'].sum())\n",
    "\n",
    "print('\\nAs a percentage:\\n')\n",
    "print('- Computer Science: ',round(train['Computer Science'].sum()/train.shape[0]*100), '%')\n",
    "print('- Physics: ',round(train['Physics'].sum()/train.shape[0]*100),'%')\n",
    "print('- Mathematics: ',round(train['Mathematics'].sum()/train.shape[0]*100),'%')\n",
    "print('- Statistics: ',round(train['Statistics'].sum()/train.shape[0]*100),'%')\n",
    "print('- Quantitative Biology: ',round(train['Quantitative Biology'].sum()/train.shape[0]*100),'%')\n",
    "print('- Quantitative Finance: ',round(train['Quantitative Finance'].sum()/train.shape[0]*100),'%')"
   ]
  },
  {
   "source": [
    "## Variable correlation?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-885504541e96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mcorrmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Draw the heatmap using seaborn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "corrmat = train.corr()\n",
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "# Draw the heatmap using seaborn\n",
    "sb.heatmap(corrmat, square=False, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Evaluation of the length distribution of Titles and Abstracts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'displot'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b134af865510>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_title_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TITLE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_title_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'displot'"
     ]
    }
   ],
   "source": [
    "train_title_len = train['TITLE'].apply(len)\n",
    "sb.displot(train_title_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'displot'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c5abbe473ff5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_abstract_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ABSTRACT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_abstract_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'displot'"
     ]
    }
   ],
   "source": [
    "train_abstract_len = train['ABSTRACT'].apply(len)\n",
    "sb.displot(train_abstract_len)"
   ]
  },
  {
   "source": [
    "# Preprocessing Techniques\n",
    "\n",
    "Be aware that each preprocessing technique applied includes some other steps such as: removing non-alphanumeric characters, lowercasing all words and tokenizing. After applying the preprocesing algorithm, there is an aditional step to remove all stopwords. After all this steps are completed, each preprocessing function returns the corpus, a structure composed of tuples encapsulating each preprocessed title-abstract pair."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Porter Stemming\n",
    "\n",
    "The Porter Stemming algorithm being the one of the oldest, originally developed in 1979, uses Suffix Stripping to produce stems. PorterStemmer is known for its simplicity and speed. It is commonly useful in Information Retrieval Environments known as IR Environments for fast recall and fetching of search queries. In a typical IR, environment documents are represented as vectors of words or terms. Words having the same stem will therefore have a similar meaning. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def porterStemming():\n",
    "\n",
    "    corpus=[]\n",
    "    # Initialize PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for i in range(0,train.shape[0]):\n",
    "        # get title and abstract and remove non alpha chars\n",
    "        title = re.sub('[^a-zA-Z]', ' ', train['TITLE'][i])\n",
    "        abstract = re.sub('[^a-zA-Z]', ' ', train['ABSTRACT'][i])\n",
    "        # to lower-case and tokenize\n",
    "        title = title.lower().split()\n",
    "        abstract = abstract.lower().split()\n",
    "        # stemming and stop word removal\n",
    "        title = ' '.join([ps.stem(w) for w in title if not w in set(stopwords.words('english'))])\n",
    "        abstract = ' '.join([ps.stem(w) for w in abstract if not w in set(stopwords.words('english'))])\n",
    "        corpus.append((title, abstract))\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "source": [
    "## Lancaster Stemming\n",
    "\n",
    "Lancaster Stemming ( also known as Paice-Husk Stemmer) was developed in 1990 and uses a more aggressive approach than the Porter Stemming Algorithm and the Snowball Algorithm, although it is not as efficient as the latter. The LancasterStemmer (Paice-Husk stemmer) is an iterative algorithm with rules saved externally. On each iteration, it tries to find an applicable rule by the last character of the word. Each rule specifies either a deletion or replacement of an ending. If there is no such rule, it terminates. It also terminates if a word starts with a vowel and there are only two letters left or if a word starts with a consonant and there are only three characters left. Otherwise, the rule is applied, and the process repeats. LancasterStemmer is simple, but heavy stemming due to iterations and over-stemming may occur, specially on smaller words. Over-stemming causes the stems to be not linguistic, or they may have no meaning. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "def lancasterStemming():\n",
    "\n",
    "    corpus=[]\n",
    "    # Initialize LancasterStemmer\n",
    "    lc = LancasterStemmer()\n",
    "\n",
    "    for i in range(0,train.shape[0]):\n",
    "        # get title and abstract and remove non alpha chars\n",
    "        title = re.sub('[^a-zA-Z]', ' ', train['TITLE'][i])\n",
    "        abstract = re.sub('[^a-zA-Z]', ' ', train['ABSTRACT'][i])\n",
    "        # to lower-case and tokenize\n",
    "        title = title.lower().split()\n",
    "        abstract = abstract.lower().split()\n",
    "        # stemming and stop word removal\n",
    "        title = ' '.join([lc.stem(w) for w in title if not w in set(stopwords.words('english'))])\n",
    "        abstract = ' '.join([lc.stem(w) for w in abstract if not w in set(stopwords.words('english'))])\n",
    "        corpus.append((title, abstract))\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "source": [
    "## Snowball Stemming\n",
    "\n",
    "When compared to the Porter Stemmer, the Snowball Stemmer can map non-English words too. Since it supports other languages the Snowball Stemmers can be called a multi-lingual stemmer. This stemmer is based on a programming language called ‘Snowball’ that processes small strings and is the most widely used stemmer. The Snowball stemmer is way more aggressive than Porter Stemmer and is also referred to as Porter2 Stemmer. Because of the improvements added when compared to the Porter Stemmer, the Snowball stemmer has greater computational speed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def snowballStemming():\n",
    "\n",
    "    corpus=[]\n",
    "    # Initialize LancasterStemmer\n",
    "    ss = SnowballStemmer(language='english')\n",
    "\n",
    "    for i in range(0,train.shape[0]):\n",
    "        # get title and abstract and remove non alpha chars\n",
    "        title = re.sub('[^a-zA-Z]', ' ', train['TITLE'][i])\n",
    "        abstract = re.sub('[^a-zA-Z]', ' ', train['ABSTRACT'][i])\n",
    "        # to lower-case and tokenize\n",
    "        title = title.lower().split()\n",
    "        abstract = abstract.lower().split()\n",
    "        # stemming and stop word removal\n",
    "        title = ' '.join([ss.stem(w) for w in title if not w in set(stopwords.words('english'))])\n",
    "        abstract = ' '.join([ss.stem(w) for w in abstract if not w in set(stopwords.words('english'))])\n",
    "        corpus.append((title, abstract))\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "source": [
    "## Some comparisons on stemming algorithms\n",
    "\n",
    "Difference Between Porter Stemmer and Snowball Stemmer:\n",
    "\n",
    "- Snowball Stemmer is more aggressive than Porter Stemmer.\n",
    "- Some issues in Porter Stemmer were fixed in Snowball Stemmer.\n",
    "- There is only a little difference in the working of these two.\n",
    "- Words like ‘fairly‘ and ‘sportingly‘ were stemmed to ‘fair’ and ‘sport’ in the snowball stemmer but when you use the porter stemmer they are stemmed to ‘fairli‘ and ‘sportingli‘.\n",
    "- The difference between the two algorithms can be clearly seen in the way the word ‘Sportingly’ in stemmed by both. Clearly Snowball Stemmer stems it to a more accurate stem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatization():\n",
    "    corpus=[]\n",
    "    # Initialize Word Net Lemmatizer\n",
    "    lemm = WordNetLemmatizer()\n",
    "\n",
    "    for i in range(0, train.shape[0]):\n",
    "        # get title and abstract and remove non alpha chars\n",
    "        title = re.sub('[^a-zA-Z]', ' ', train['TITLE'][i])\n",
    "        abstract = re.sub('[^a-zA-Z]', ' ', train['ABSTRACT'][i])\n",
    "        # to lower-case and tokenize\n",
    "        title = title.lower().split()\n",
    "        abstract = abstract.lower().split()\n",
    "        # lemmatization and stop word removal\n",
    "        title = ' '.join([lemm.lemmatize(w) for w in title if not w in set(stopwords.words('english'))])\n",
    "        abstract = ' '.join([lemm.lemmatize(w) for w in abstract if not w in set(stopwords.words('english'))])\n",
    "        corpus.append((title, abstract))\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "source": [
    "## Stemming vs. Lemmatization\n",
    "\n",
    "Stemming and Lemmatization are itself form of NLP and widely used in Text mining. Text Mining is the process of analysis of texts written in natural language and extract high-quality information from text. It involves looking for interesting patterns in the text or to extract data from the text to be inserted into a database. Developers have to prepare text using lexical analysis, POS (Parts-of-speech) tagging, stemming and other Natural Language Processing techniques to gain useful information from text.\n",
    "\n",
    "### When should one use Stemming or Lemmatization?\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, using WordNet corpus and a corpus for stop words as well to produce lemma, which makes it slower than stemming.\n",
    "\n",
    "The above points show that if speed is the focus then stemming should be used since lemmatizers scan a corpus which consumes time and processing. It widely depends on the problem characteristics if stemmers should be used or lemmatizers.\n",
    "\n",
    "### Pros of Stemming:\n",
    "\n",
    "### Cons of Stemming:\n",
    "- Issues of over stemming and under stemming may lead to not so meaningful or inappropriate stems.\n",
    "- Stemming does not consider how the word is being used. For example – the word ‘saw‘ will be stemmed to ‘saw‘ itself but it won’t be considered whether the word is being used as a noun or a verb in the context. For this reason, Lemmatization is used as it keeps this fact in consideration and will return either ‘see’ or ‘saw’ depending on whether the word ‘saw’ was used as a verb or a noun.\n",
    "\n",
    "\n",
    "### Pros of Lemmatization:\n",
    "\n",
    "### Cons of Lemmatization:\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Bag-of-words\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "After forming the corpus, the bag-of-words model can be built!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def bagOfWords(corpus):\n",
    "    # Create bag-of-words model\n",
    "    data = []\n",
    "    for (title, abstract) in corpus:\n",
    "        data.append(title + abstract)\n",
    "\n",
    "    vectorizer = CountVectorizer(max_features = 1500)\n",
    "    X = vectorizer.fit_transform(data).toarray()\n",
    "    y = train.iloc[:,-1].values\n",
    "\n",
    "    # print(X.shape, y.shape)\n",
    "    # print(vectorizer.get_feature_names())\n",
    "    return X,y\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Splitting the training dataset \n",
    "\n",
    "In order to create a test dataset where the accuracy of the models can be evaluated, the training dataset must be split into a smaller training dataset and a test dataset, in order to compare the model predictions to the correct topic modelling answer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def splittingDataset(X,y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "    # print(X_train.shape, y_train.shape)\n",
    "    # print(X_test.shape, y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "source": [
    "## SVM\n",
    "\n",
    "In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier. SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n",
    "\n",
    "In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def SVM(X_train, y_train, X_test):\n",
    "    classifier = SVC()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision tree learning is one of the predictive modelling approaches used in statistics, data mining and machine learning. It uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "# import graphviz\n",
    "\n",
    "def decisionTree(X_train, y_train, X_test):\n",
    "    classifier = DecisionTreeClassifier(min_samples_leaf=10)\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #Visualizing the decision tree\n",
    "    # dot_data = tree.export_graphviz(model, feature_names=list(X), class_names=sorted(y.unique()), filled=True)\n",
    "    # graphviz.Source(dot_data)\n",
    "\n",
    "    return y_pred \n"
   ]
  },
  {
   "source": [
    "## Naïve Bayes\n",
    "\n",
    "Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes theorem and used for solving classification problems. It is a probabilistic classifier, which means it predicts on the basis of the probability of an object. It is mainly used in text classification that includes a high-dimensional training dataset. This algorithm is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def naiveBayes(X_train, y_train, X_test):\n",
    "    classifier = GaussianNB()\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "source": [
    "# Running the code\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluatePerformance(y_test, y_pred):\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred))\n",
    "    print('Recall: ', recall_score(y_test, y_pred))\n",
    "    print('F1: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "## SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### SVM with Porter Stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-38391eaffb57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mporterStemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbagOfWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittingDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mevaluatePerformance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-5e47939472f7>\u001b[0m in \u001b[0;36mporterStemming\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# stemming and stop word removal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mabstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mabstract\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabstract\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-5e47939472f7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# stemming and stop word removal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mabstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mabstract\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabstract\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Programs\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \"\"\"\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[0;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Programs\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Programs\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Programs\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Programs\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, encoding)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeekableUnicodeStreamReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus = porterStemming()\n",
    "X,y = bagOfWords(corpus)\n",
    "X_train, X_test, y_train, y_test = splittingDataset(X,y)\n",
    "y_pred = SVM(X_train, y_train, X_test)\n",
    "evaluatePerformance(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "### SVM with Lancaster Stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = lancasterStemming()\n",
    "X,y = bagOfWords(corpus)\n",
    "X_train, X_test, y_train, y_test = splittingDataset(X,y)\n",
    "y_pred = SVM(X_train, y_train, X_test)\n",
    "evaluatePerformance(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "### SVM with Snowball Stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[4144    1]\n [  50    0]]\nAccuracy:  0.9878426698450536\nPrecision:  0.0\nRecall:  0.0\nF1:  0.0\n"
    }
   ],
   "source": [
    "corpus = snowballStemming()\n",
    "X,y = bagOfWords(corpus)\n",
    "X_train, X_test, y_train, y_test = splittingDataset(X,y)\n",
    "y_pred = SVM(X_train, y_train, X_test)\n",
    "evaluatePerformance(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "### SVM with Lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[4145    0]\n [  50    0]]\nAccuracy:  0.9880810488676997\nPrecision:  0.0\nRecall:  0.0\nF1: 0.0\nc:\\Programs\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n  'precision', 'predicted', average, warn_for)\nc:\\Programs\\WPy-3662\\python-3.6.6.amd64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n  'precision', 'predicted', average, warn_for)\n"
    }
   ],
   "source": [
    "corpus = lemmatization()\n",
    "X,y = bagOfWords(corpus)\n",
    "X_train, X_test, y_train, y_test = splittingDataset(X,y)\n",
    "y_pred = SVM(X_train, y_train, X_test)\n",
    "evaluatePerformance(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "## Decision Trees"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Decision Trees with Porter Stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = porterStemming()\n",
    "X,y = bagOfWords(corpus)\n",
    "X_train, X_test, y_train, y_test = splittingDataset(X,y)\n",
    "y_pred = decisionTree(X_train, y_train, X_test)\n",
    "evaluatePerformance(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "### Decision Trees with Lancaster Stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[4130   15]\n [  35   15]]\nAccuracy:  0.9880810488676997\nPrecision:  0.5\nRecall:  0.3\nF1:  0.37499999999999994\n"
    }
   ],
   "source": [
    "corpus = lancasterStemming()\n",
    "X,y = bagOfWords(corpus)\n",
    "X_train, X_test, y_train, y_test = splittingDataset(X,y)\n",
    "y_pred = decisionTree(X_train, y_train, X_test)\n",
    "evaluatePerformance(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "### Decision Trees with Snowball Stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[4133   12]\n [  42    8]]\nAccuracy:  0.9871275327771156\nPrecision:  0.4\nRecall:  0.16\nF1:  0.22857142857142856\n"
    }
   ],
   "source": [
    "corpus = snowballStemming()\n",
    "X,y = bagOfWords(corpus)\n",
    "X_train, X_test, y_train, y_test = splittingDataset(X,y)\n",
    "y_pred = decisionTree(X_train, y_train, X_test)\n",
    "evaluatePerformance(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "### Decision Trees with Lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = lemmatization()\n",
    "X,y = bagOfWords(corpus)\n",
    "X_train, X_test, y_train, y_test = splittingDataset(X,y)\n",
    "y_pred = decisionTree(X_train, y_train, X_test)\n",
    "evaluatePerformance(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "## Naïve Bayes\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes with Porter Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = porterStemming()\n",
    "X,y = bagOfWords(corpus)\n",
    "X_train, X_test, y_train, y_test = splittingDataset(X,y)\n",
    "y_pred = decisionTree(X_train, y_train, X_test)\n",
    "evaluatePerformance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes with Lancaster Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = lancasterStemming()\n",
    "X,y = bagOfWords(corpus)\n",
    "X_train, X_test, y_train, y_test = splittingDataset(X,y)\n",
    "y_pred = decisionTree(X_train, y_train, X_test)\n",
    "evaluatePerformance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes with Snowball Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = snowballStemming()\n",
    "X,y = bagOfWords(corpus)\n",
    "X_train, X_test, y_train, y_test = splittingDataset(X,y)\n",
    "y_pred = naiveBayes(X_train, y_train, X_test)\n",
    "evaluatePerformance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes with Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = lemmatization()\n",
    "X,y = bagOfWords(corpus)\n",
    "X_train, X_test, y_train, y_test = splittingDataset(X,y)\n",
    "y_pred = naiveBayes(X_train, y_train, X_test)\n",
    "evaluatePerformance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Comparison Between Different Approaches"
   ]
  },
  {
   "source": [
    "# References\n",
    "\n",
    "\n",
    "- https://www.geeksforgeeks.org/introduction-to-stemming/\n",
    "- https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "- https://en.wikipedia.org/wiki/Support-vector_machine\n",
    "- https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "- https://www.javatpoint.com/machine-learning-naive-bayes-classifier\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "53423842328b7235b2a141629a327cacca762685d220111c61b1c436afc9e911"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}